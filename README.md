Titanic Survival Classification

Overview

This project implements a machine learning model to predict survival outcomes for passengers on the Titanic using the Kaggle Titanic dataset. The model uses the XGBoost classifier to predict whether a passenger survived based on features such as age, sex, fare, and family size.

Dataset

The dataset is sourced from Kaggle's Titanic: Machine Learning from Disaster competition. It consists of:





train.csv: Training data with passenger details and survival outcomes.



test.csv: Test data with passenger details (without survival outcomes).

Prerequisites

To run this project, you need the following dependencies:





Python 3.6+



pandas



numpy



scikit-learn



xgboost

You can install the required packages using:

pip install pandas numpy scikit-learn xgboost

Project Structure





Titanic Survival Classification.ipynb: The main Jupyter Notebook containing the data preprocessing, feature engineering, model training, and prediction code.



train.csv: Training dataset (not included in the repository; download from Kaggle).



test.csv: Test dataset (not included in the repository; download from Kaggle).



submission.csv: Output file containing predictions for the test set (generated by the notebook).

How to Run





Clone the Repository:

git clone https://github.com/your-username/titanic-survival-classification.git
cd titanic-survival-classification



Download the Dataset:





Download train.csv and test.csv from the Kaggle Titanic competition.



Place both files in the project directory.



Run the Notebook:





Open the Jupyter Notebook:

jupyter notebook Titanic\ Survival\ Classification.ipynb



Execute the cells in the notebook sequentially to preprocess the data, train the model, and generate predictions.



The notebook will output a submission.csv file with predictions for the test set.

Methodology





Data Preprocessing:





Missing values in Age, Fare, and Embarked are filled with median or mode values.



Categorical variables (Sex, Embarked, Title) are encoded using LabelEncoder.



Features like Cabin, Ticket, and Name are dropped due to high missing values or irrelevance.



New features (Title, FamilySize, IsAlone) are engineered to improve model performance.



Model Training:





The dataset is split into training (80%) and validation (20%) sets.



An XGBoost classifier is trained with logloss as the evaluation metric.



Validation accuracy is computed to evaluate model performance.



Prediction:





The trained model predicts survival outcomes for the test set.



Predictions are saved to submission.csv in the format required by Kaggle.

Results





Validation Accuracy: Approximately 80.45% (as shown in the notebook output).



The generated submission.csv can be submitted to Kaggle to evaluate performance on the test set.

Future Improvements





Experiment with hyperparameter tuning for the XGBoost model.



Explore additional feature engineering, such as extracting more information from the Ticket or Cabin columns.



Try other machine learning models (e.g., Random Forest, Neural Networks) for comparison.



Implement cross-validation for more robust performance evaluation.

License

This project is licensed under the MIT License. See the LICENSE file for details.

Acknowledgments





Kaggle for providing the Titanic dataset.



The open-source community for libraries like pandas, scikit-learn, and xgboost.
